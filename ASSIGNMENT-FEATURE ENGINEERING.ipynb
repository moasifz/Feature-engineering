{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "# Q-1 What is a parameter?\n#ANS- A parameter is a variable or constant that defines certain characteristics or conditions in a specific system, process, or model.\n#Parameters are typically used to set constraints, limit values, or influence the behavior of a system.\n#Here are a few contexts where \"parameter\" is commonly used:\n#Mathematics/Statistics: A parameter is a numerical factor that defines a particular characteristic of a function, distribution, or system.\n#For example, in a normal distribution, the mean and standard deviation are parameters that define the shape of the distribution.\n#Programming/Software Development: In programming, a parameter is a variable used in a function or method definition. It accepts a value when the function is called",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q-2 What is correlation?\n#What does negative correlation mean?",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#ANS-Correlation refers to the statistical relationship or association between two or more variables.\n#It describes whether and how strongly the variables move together. In other words, correlation indicates if changes in one variable are related to changes in another.\n#A positive correlation means that as one variable increases, the other tends to increase as well.\n#A negative correlation means that as one variable increases, the other tends to decrease.\n\n#Negative correlation refers to a relationship between two variables where, as one variable increases,\n#the other variable decreases, and vice versa. This type of relationship is often referred to as an inverse relationship.\n\n#Key Features of Negative Correlation:\n#If one variable goes up, the other tends to go down.\n#The correlation coefficient (r) for a negative correlation ranges between -1 and 0",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#3-Define machine learning.what are the main components in machine learning?\n#ANS-Machine learning is a subset of artificial intelligence (AI) that focuses on building algorithms and statistical models\n#that allow computers to learn from and make predictions or decisions based on data,without being explicitly programmed to perform specific tasks.\n# Main Components of Machine Learning\n#Data (Training and Test data)\n#Algorithms (Methods to extract patterns)\n#Model (Trained representation of the data)\n#Features (Input variables or characteristics)\n#Learning Process (Supervised, Unsupervised, Reinforcement)\n#Evaluation Metrics (Measuring model performance)\n#Optimization (Improving model's predictions)\n#Hyperparameters (Settings influencing training)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": "#Q-4 How does loss value help in determining whether the model is good or not?\n#ANS-The loss value (also known as the loss function or cost function) is a key indicator used to measure how well a machine\n#learning model is performing. It quantifies the difference between the model\n#predictions and the actual values (true labels) in the data. The goal of training a model is to minimize this loss,\n#indicating that the model is making predictions that are closer to the true values.\n\n#Loss indicates model performance: Lower loss suggests better predictions.\n#Guides optimization: During training, minimizing loss improves model accuracy.\n#Reveals overfitting or underfitting: Loss on training vs. test data highlights generalization issues.\n#Assists in model selection and hyperparameter tuning: Comparing models or adjusting settings is done based on loss values.\n#Interpretable feedback: Provides clear guidance on whether the model is performing well or needs improvement.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q-5 What are continuous and categorical variables?\n#ANS-Continuous Variables and Categorical Variables are two types of data that differ in how they can be measured and represented.\n# 1. Continuous Variables:\n# Definition: Continuous variables are quantitative variables that can take any value within a given range.\n#They can represent measurements and can be divided into smaller increments.\n# Characteristics:\n# They are numeric and can have an infinite number of possible values.\n#Examples: Height, weight, temperature, age, time, distance.Can take decimal values (e.g., 2.5, 3.14).Often measured on a continuous scale.\n#Example:The temperature in a room can be 20.5°C, 20.55°C, or 20.555°C, and so on, showing an infinite number of possible values within a range.\n#2. Categorical Variables:\n#Definition: Categorical variables, also known as qualitative variables, represent data that can be divided into distinct categories or groups.\n#The values are not numeric but represent different classes or categories.\n#Characteristics:\n#They can take on a limited, fixed number of categories.\n#Nominal: Categories with no inherent order (e.g., colors, countries, or brands).\n#rdinal: Categories with a clear, meaningful order but the distance between them isn’t consistent (e.g., education levels\n#like \"High School,\" \"Bachelor's,\" and \"Master's\").",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q-6How do we handle categorical variables in Machine learning?What are the common techniques?\n#ANS-Handling categorical variables is an essential part of preprocessing data for machine learning.\n#Most machine learning algorithms require numerical input, so categorical variables (which represent distinct classes or categories)\n#need to be converted into a format that the model can understand.\n\n#1. Label Encoding:\n#What it is: Label encoding involves converting each category into a unique integer. Each category is assigned a number, usually starting from 0.\n#When to use: Label encoding is useful when the categorical variable has a natural order or ranking (ordinal variable). \n#For example, categories like Low, Medium, and High can be represented as 0, 1, and 2.\n# Example\nCategory: [\"Low\", \"Medium\", \"High\"]\nEncoded: [0, 1, 2]\n\n\n#2 One-Hot Encoding:\n#What it is: One-hot encoding transforms each category of a variable into a separate binary (0 or 1) column. For each category,\n#a new column is created, and if the category is present in a row, it gets a value of 1; otherwise, it gets a value of 0.\n#When to use: One-hot encoding is used when the categorical variable is nominal, i.e., the categories have no inherent order.\n# Example\n\nCategory: [\"Red\", \"Blue\", \"Green\"]\nOne-Hot Encoded:\nRed   | Blue | Green\n-------------------\n1     | 0    | 0\n0     | 1    | 0\n0     | 0    | 1\n\n\n#Advantages: It avoids introducing any ordinal relationships between categories, which is important for nominal data.\n#Limitations: It increases the dimensionality of the dataset, which can lead to sparse data and potential performance issues,\n#especially with many unique categories.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q-7What do you mean by training and testing a dataset?\n#ANS-In machine learning, the terms training and testing a dataset refer to the processes of training a model on one part of\n#the data and evaluating its performance on another, separate part of the data.\n#1 Training a Dataset:\n#What it means: Training a model involves using a subset of the data (called the training set) to teach the model how to make predictions or classifications.\n#Process:\n#The training dataset is used to \"teach\" the model by feeding it input data (features) along with the correct output (labels or targets). \n#The model learns the relationships between the inputs and outputs by adjusting its parameters (e.g., weights in a neural network or splits in a decision tree) to minimize the error or loss.\n#Goal: The goal of training is for the model to learn the underlying patterns and relationships in the data so that \n#it can make accurate predictions on new, unseen data.\n\n#2. Testing a Dataset:\n#What it means: Testing the model involves using a separate subset of the data, known as the test set, to evaluate how well the model performs after it has been trained.\n#Process:\n#Once the model is trained on the training data, it is evaluated on the test data (which it has not seen during training).\n#The test set serves as a proxy for unseen data and helps to assess how well the model generalizes beyond the specific examples it was trained on.\n#Goal: The goal of testing is to measure the generalization ability of the model — how well it can perform on new,\n#previously unseen data.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q -8 OR Q-23 ARE SAME.\n#Q-8 What is sklearn.preprocessing?\n#ANS-sklearn.preprocessing is a module in scikit-learn (a popular Python library for machine learning) that provides a\n#variety of tools for data preprocessing. Preprocessing refers to the steps performed on raw data before feeding it into machine learning models. \n#This module contains methods for transforming features to make them suitable for machine learning algorithms.\n# Encoding Categorical Features\n#LabelEncoder: Converts categorical labels (target variables) into numerical values. It is mainly used for target variables \n#in classification tasks.\n#For example, it converts labels like ['cat', 'dog', 'fish'] to [0, 1, 2].\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny_encoded = le.fit_transform(y)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": "#Q-9What is a test set?\n#ANS-A test set is a subset of your dataset that is used to evaluate the performance of a machine learning model after it \n#has been trained. The key purpose of the test set is to assess how well the model generalizes to unseen data, providing an\n#indication of how the model is likely to perform on new, real-world data.\n\n#Key Characteristics of a Test Set:\n#Not Used in Training: The test set is completely separate from the data used to train the model. The model never sees or\n#learns from the test set during the training process. This ensures an unbiased evaluation.\n#Evaluates Generalization: The main goal of the test set is to measure how well the model can generalize to new, unseen data.\n#This is important because the ultimate goal of a machine learning model is to make accurate predictions on data it hasn't encountered before.\n#Represents Real-World Data: Since the test set is treated as \"new\" data, it simulates how the model will perform when \n#deployed in real-world scenarios, where the data it encounters may differ from the training data.\n#Contains Labels: The test set typically includes true labels (the correct answers or target values) for supervised learning\n#tasks, which are needed to evaluate the model's predictions and compute performance metrics.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q-10-How do we split data for model fitting (training and testing) in python?\n#How do you approach a Machine learning problem?\n#ANS-In Python, particularly using scikit-learn (a popular machine learning library), splitting data into training and testing\n#sets is done using the train_test_split function from the model_selection module. This function randomly splits the data \n#into two subsets: one for training the model and one for evaluating it.\n# Example\n\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Example data\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])  # Features\ny = np.array([1, 2, 3, 4, 5])  # Target variable\n\n# Split the data into training and testing sets (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(\"Training Features:\", X_train)\nprint(\"Test Features:\", X_test)\nprint(\"Training Target:\", y_train)\nprint(\"Test Target:\", y_test)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Training Features: [[ 9 10]\n [ 5  6]\n [ 1  2]\n [ 7  8]]\nTest Features: [[3 4]]\nTraining Target: [5 3 1 4]\nTest Target: [2]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 13
    },
    {
      "cell_type": "code",
      "source": "#How do you approach a Machine Learning problem?\n#Approaching a machine learning problem typically involves a series of systematic steps to ensure that you understand th\n#problem, preprocess the data correctly, build a good model, and evaluate its performance effectively. Here’s a common step-by-step approach.\n\n#1-Define the problem and understand the objective.\n#2-Collect and prepare the data: Perform data cleaning, transformation, and feature engineering.\n#3-Split the data into training and testing sets.\n#4-Choose the model based on the problem type (classification, regression, clustering, etc.).\n#5-Train the model on the training data.\n#6-Evaluate the model on the test data using appropriate metrics.\n#7-Tune the model using hyperparameter optimization and cross-validation.\n#8-Deploy and monitor the model once it’s ready for real-world use.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q-11Why do we have to perform EDA before fitting a model to the data?\n#ANS- Performing Exploratory Data Analysis (EDA) before fitting a model to the data is a critical step in the machine learning\n#pipeline. EDA helps you better understand the characteristics of your dataset, which guides the subsequent steps in the\n#modeling process. Here's why EDA is essential.\n#Performing Exploratory Data Analysis (EDA) is crucial because it helps you:\n\n#1-Understand the data structure, distribution, and relationships between features and the target.\n#2-Identify data quality issues like missing values, outliers, and inconsistencies.\n#30Prepare the data by transforming, scaling, and encoding features as necessary.\n#4-Decide on the right model based on the nature of the data (e.g., regression, classification).\n#5-Discover new features or interactions that may improve the model.\n#6-Build trust in the model by understanding the data deeply before applying machine learning algorithms.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q-12What is correlation?\n#Correlation is a statistical measure that describes the strength and direction of the relationship between two variables.\n#It tells us how one variable changes in relation to another. If two variables tend to increase or decrease together, \n#they are said to be correlated.\n\n#Types of Correlation:\n#1- Positive Correlation: When two variables increase or decrease together in the same direction. For example, as the temperature increases, the sales of ice cream may also increase.\n\n#Example: Height and weight often show a positive correlation—taller people tend to weigh more.\n#2- Negative Correlation: When one variable increases while the other decreases, or vice versa. In other words, they move in opposite directions.\n#Example: The amount of time spent on exercise and body fat percentage might show a negative correlation — as the time spent on exercise increases, the body fat percentage may decrease.\n\n#3- No Correlation: When there is no discernible relationship between the two variables, meaning changes in one variable \n#do not predict changes in the other.\n#example: The number of books a person reads and the number of cars they own might show no correlation.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q-13What does neagative correlation mean?\n#ANS-Negative correlation refers to a relationship between two variables in which one variable increases as the other\n#decreases, or vice versa. Essentially, when one variable goes up, the other tends to go down, and when one variable goes down,\n#the other tends to go up. This kind of relationship is called inverse correlation.\n\n#Key Points:\n#Inverse relationship: Negative correlation means that the variables move in opposite directions.\n#Correlation coefficient: A negative correlation is represented by a correlation coefficient(r)\n#that falls between -1 and 0. The closer the value is to -1, the stronger the negative correlation.\n#Examples of Negative Correlation:\n#1-Exercise and body fat percentage  2-Price and demand  3-Temperature and heating costs.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#14-How can you find the correlation between variables in python?\n#ANS-To find the correlation between variables in Python, you can use libraries such as Pandas, NumPy, or SciPy.\n#Here's how to compute correlation using these libraries.\n#. Using Pandas\n#Pandas provides a simple and efficient way to compute correlation using the .corr() method, which computes the Pearson \n#correlation by default.\nimport pandas as pd\n\n# Example data\ndata = {'Height': [150, 160, 170, 180, 190],\n        'Weight': [50, 60, 70, 80, 90]}\n\n# Create a DataFrame\ndf = pd.DataFrame(data)\n\n# Calculate correlation between the variables\ncorrelation = df.corr()\n\n# Print the correlation matrix\nprint(correlation)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "<ipython-input-14-76c8718b0aa1>:7: DeprecationWarning: \nPyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\nbut was not found to be installed on your system.\nIf this would cause problems for you,\nplease provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n        \n  import pandas as pd\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "        Height  Weight\nHeight     1.0     1.0\nWeight     1.0     1.0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "code",
      "source": "#Q-15What is causation?Explain difference between correlation and causation with an example.\n#ANS-Causation refers to a cause-and-effect relationship between two variables, where one variable directly influences the\n#other. In other words, a change in one variable leads to a change in the other variable, and the effect can be traced back to the cause.\n\n#Difference Between Correlation and Causation:\n#1. Correlation:\n#Definition: Correlation measures the association between two variables. If two variables are correlated, \n#it means that they tend to move together, but it does not imply that one is causing the other to change.\n#Nature of relationship: Correlation can be either positive or negative, but it does not specify a direct cause-and-effect relationship.\n#Interpretation: Correlation tells us how two variables are related but not why.\n#2. Causation:\n#Definition: Causation is a direct cause-and-effect relationship where one variable directly influences or causes the change in another variable.\n#Nature of relationship: Causation specifies that one variable causes another to change.\n#Interpretation: Causation tells us not just how two variables are related, but also why and how one causes the other to change.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q-16 What is an Optimizer? What are different types of optimizers? Explain each with an example.\n#ANS-An optimizer in machine learning is an algorithm or method used to minimize or maximize an objective function,\n#such as a loss function (for training models), by adjusting the parameters (weights and biases) of the model. \n\n#Different Types of Optimizers\n#There are several types of optimizers, each with different approaches to updating the model parameters.\n#Below are the most commonly used optimizers in machine learning.\n#1. Gradient Descent (GD)\n#Description: Gradient Descent is the simplest optimization algorithm. It calculates the gradient of the loss function with\n#respect to each parameter and updates the parameters in the opposite direction of the gradient. \n#The step size is determined by the learning rate.\n#Example: If you're training a linear regression model, gradient descent will adjust the weights w of the linear equation\n#y=wx+b to minimize the mean squared error (MSE) loss.\n#Types:\n#Batch Gradient Descent: Uses the entire dataset to compute the gradient.\n#Stochastic Gradient Descent (SGD): Uses one random sample to compute the gradient at each step.\n#Mini-batch Gradient Descent: Uses a small random batch of samples to compute the gradient.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q-17 What is sklearn.linear_model?\n#ANS- sklearn.linear_model is a module in scikit-learn, a popular machine learning library in Python, that contains various\n#linear models for regression and classification tasks. These models are based on the principles of linear relationships \n#between input variables and the target variable.\n#Key Models in sklearn.linear_model:\n#Linear Regression (LinearRegression)\n#Purpose: Used for regression tasks where the relationship between the independent variables (features) and the \n#dependent variable (target) is assumed to be linear.\n#Example: Predicting house prices based on features like size, location, etc.\n#Example Usage:\n\nfrom sklearn.linear_model import LinearRegression\n\n# Example data\nX = [[1], [2], [3], [4]]  # Features (input)\ny = [1, 2, 3, 4]  # Target (output)\n\nmodel = LinearRegression()\nmodel.fit(X, y)\n\nprint(model.predict([[5]]))  # Predict for new data",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[5.]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 15
    },
    {
      "cell_type": "code",
      "source": "#Q-18-What does model.fit()do?What arguementsmust be given?\n#ANS-#In machine learning, the model.fit() method is used to train a model on the provided dataset. When you call model.fit(),\n#you're fitting the model to the data by adjusting the model's parameters (such as weights and biases in the case of linear\n#models or neural networks). \n\n#What Does model.fit() Do?\n#Learning from Data: It takes the training data and the corresponding labels (for supervised learning) and finds the best-fit parameters that minimize the error (e.g., loss function).\n#Model Training: The model's internal parameters (e.g., coefficients in linear regression, weights in neural networks) are adjusted using an optimization algorithm like gradient descent to minimize the error.\n#Fitting Process: The process of fitting is essentially training, where the model learns the relationship between features (input) and the target (output).\n\n#Arguments for model.fit()\n#The arguments you need to pass to model.fit() depend on the type of model and task you are dealing with (e.g., regression or classification). Generally, two main arguments are required:\n#X (features): This is the input data. It consists of the features or independent variables that will be used to predict the target variable. It is typically a 2D array or DataFrame, where rows represent data points (samples) and columns represent features.\n#Shape: (n_samples, n_features) — n_samples is the number of data points, and n_features is the number of features or input variables.\n#y (target): This is the output or dependent variable. It contains the labels or target values that correspond to the input data. For supervised learning, this is what you want the model to predict.\n#Shape: (n_samples,) for a 1D array of labels in classification or regression tasks, or (n_samples, n_targets) for multi-output tasks.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q-19 What does model.predict()do?What arguements must be given?\n#ANS-The model.predict() method in machine learning is used to make predictions after a model has been trained (fitted).\n\n#What Does model.predict() Do?\n#Prediction: After the model has been trained using model.fit(), you can call model.predict() to get the model’s output for \n#new data points. It applies the learned parameters (like coefficients in linear models or weights in neural networks) to the input data and produces predictions.\n#Inference: It makes an inference based on the input features, which means it predicts what the tthe target variable (output)\n#would be for unseen data, based on the relationships the model has learned during training.\n\n#Arguments for model.predict()\n#The main argument you need to provide to model.predict() is the input data for which you want the model to make predictions.\n#This input data must have the same structure and format as the data used to train the model.\n#X (features):\n#This is the new data you want the model to predict on. It should be in the same format as the data passed to model.fit().\n#Shape: (n_samples, n_features) — n_samples is the number of new samples, and n_features is the number of features. \n#Each sample represents a data point, and each feature corresponds to an attribute or characteristic of that data point.\n#The number of features in the prediction data must match the number of features used during training (i.e., \n#the number of columns in X used for training should be the same as the number of columns in the data passed to pre\n\n#Here is an example using linear regression to illustrate how model.predict() works:\n\nfrom sklearn.linear_model import LinearRegression\n\n# Training data (features and target)\nX_train = [[1], [2], [3], [4]]\ny_train = [1, 2, 3, 4]\n\n# Create and train the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# New data for prediction\nX_new = [[5], [6]]\n\n# Make predictions on the new data\npredictions = model.predict(X_new)\n\nprint(predictions)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[5. 6.]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 16
    },
    {
      "cell_type": "code",
      "source": "#Q-20 What are continuous and categorical variables?\n#ANS-In statistics and machine learning, continuous and categorical variables are two types of data based on the nature of\n#the values they can take and how those values are interpreted.\n#1. Continuous Variables\n#Definition: Continuous variables are numeric variables that can take any value within a range.\n#These values are typically real numbers and can have decimal places, representing quantities or measurements.\n#Examples:\n#Height of a person (e.g., 170.5 cm) ,Temperature (e.g., 36.6°C) ,Weight (e.g., 68.2 kg) ,Income (e.g., $45,000.50)\n#Characteristics:\n#Values are ordered and measurable.\n#They can take infinitely many values within a specified range.\n#Often associated with physical measurements or quantities.\n#Use in Machine Learning:\n#Used directly in most machine learning models without transformation.\n#Examples include features like age, salary, or temperature.\n\n#2. Categorical Variables\n#Definition: Categorical variables represent discrete groups or categories. The values of these variables are labels\n#or names that signify membership in a particular group.\n#Examples: Gender (e.g., Male, Female, Other), Color (e.g., Red, Blue, Green), Type of vehicle (e.g., Car, Truck, Bike)\n#Education level (e.g., High School, Bachelor’s, Master’s), Types of Categorical Variables:\n\n#Nominal: Categories have no inherent order.\n#Example: Colors (Red, Green, Blue), Gender (Male, Female, Other)\n#Ordinal: Categories have a meaningful order or ranking.\n#Example: Education level (High School < Bachelor’s < Master’s)\n#Characteristics:\n#Values are distinct groups or labels.\n#They cannot be measured on a continuous scale.\n#Can be numeric (e.g., 0 for Male, 1 for Female) but these numbers do not have mathematical significance.\n#Use in Machine Learning:\n#Need to be converted into numeric representations (e.g., one-hot encoding or label encoding) before being used in most algorithms.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q-21 What is feature scaling?How does it help in Machine learning?\n#ANS-Feature Scaling is a preprocessing technique used in machine learning to standardize or normalize the range of \n#independent variables (features). The purpose is to ensure that all features have a comparable scale, preventing features\n#with larger ranges from dominating the model's learning process.\n\n#It helps in machine learning.\n#In many machine learning algorithms, the magnitude of feature values can significantly impact model performance. \n#Here's why scaling is important:\n#Equal Contribution:\n#Features with larger scales (e.g., salary in thousands) can dominate features with smaller scales (e.g., age in years), leading to biased models.\n#Feature scaling ensures all features contribute equally.\n#Improved Convergence:\n\n#For optimization algorithms (like Gradient Descent), having features on similar scales accelerates convergence and prevents numerical instability.\n#Sensitive Models:\n#Certain machine learning algorithms are sensitive to feature magnitudes. These include:\n#Distance-based algorithms: K-Nearest Neighbors (KNN), Support Vector Machines (SVMs), and clustering (e.g., K-Means) rely on distances like Euclidean distance, which is affected by feature scales.\n#Gradient-based algorithms: Linear Regression, Logistic Regression, Neural Networks, etc., benefit from scaled features for better performance.\n#Decision Tree-based models (e.g., Random Forest, Gradient Boosted Trees) are generally not affected by feature scaling",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#Q-22 How do we perform scaling in python?\n#ANS-Scaling in Python typically involves transforming numerical data to a specific range or distribution, making it more \n#suitable for machine learning models or other analysis. This is commonly done using libraries like scikit-learn, \n#which provides utilities for scaling data. Here’s a quick overview of the most popular scaling methods and their implementations.\n\n#1. Standardization (Z-Score Scaling)\n#Transforms the data to have a mean of 0 and a standard deviation of 1.\n\nfrom sklearn.preprocessing import StandardScaler\n\n# Example data\ndata = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(data)\n\nprint(scaled_data)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[[-1.22474487 -1.22474487 -1.22474487]\n [ 0.          0.          0.        ]\n [ 1.22474487  1.22474487  1.22474487]]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": "#2. Min-Max Scaling\n#Transforms data to a fixed range, usually [0, 1].\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(data)\n\nprint(scaled_data)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[[0.  0.  0. ]\n [0.5 0.5 0.5]\n [1.  1.  1. ]]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": "#Q-24How do we split data for model fitting(training and testing) in python?\n#ANS-In Python, particularly using scikit-learn (a popular machine learning library), splitting data into training and testing\n#sets is done using the train_test_split function from the model_selection module. This function randomly splits the data \n#into two subsets: one for training the model and one for evaluating it.\n# Example\n\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Example data\nX = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])  # Features\ny = np.array([1, 2, 3, 4, 5])  # Target variable\n\n# Split the data into training and testing sets (80% training, 20% testing)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(\"Training Features:\", X_train)\nprint(\"Test Features:\", X_test)\nprint(\"Training Target:\", y_train)\nprint(\"Test Target:\", y_test)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Training Features: [[ 9 10]\n [ 5  6]\n [ 1  2]\n [ 7  8]]\nTest Features: [[3 4]]\nTraining Target: [5 3 1 4]\nTest Target: [2]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": "#Q-25Explain data encoding?\n#ANS-Data encoding is the process of converting categorical or textual data into numerical formats that machine learning\n#models can understand and process. Since most ML models work with numerical data, encoding ensures that categorical features\n#(e.g., \"red,\" \"blue,\" \"green\") are transformed into a suitable format for analysis while preserving the information.\n\n#Types of Data Encoding\n#1- Label Encoding\n#Assigns a unique integer to each category.\n#Useful for ordinal data (where categories have a natural order).\n#Example:\nfrom sklearn.preprocessing import LabelEncoder\n\ndata = ['red', 'blue', 'green']\nencoder = LabelEncoder()\nencoded_data = encoder.fit_transform(data)\n\nprint(encoded_data) ",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[2 0 1]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 9
    },
    {
      "cell_type": "code",
      "source": "#2-One-Hot Encoding\n#Converts each category into a binary column, with a 1 indicating the presence of that category.\n#Commonly used for nominal data (categories with no inherent order).\n#Example:\nfrom sklearn.preprocessing import OneHotEncoder\n\ndata = [['red'], ['blue'], ['green']]\nencoder = OneHotEncoder()\nencoded_data = encoder.fit_transform(data).toarray()\n\nprint(encoded_data)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[[0. 0. 1.]\n [1. 0. 0.]\n [0. 1. 0.]]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": "#3-Ordinal Encoding\n#Similar to label encoding but allows manual mapping for ordinal relationships.\n#Example:\nfrom sklearn.preprocessing import OrdinalEncoder\n\ndata = [['low'], ['medium'], ['high']]\nencoder = OrdinalEncoder(categories=[['low', 'medium', 'high']])\nencoded_data = encoder.fit_transform(data)\n\nprint(encoded_data)",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[[0.]\n [1.]\n [2.]]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 12
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}